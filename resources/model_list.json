[
	{
        "model": "Qwen2-VL 2B (6bit)",
        "config": "qwen2-vl-2b-q6.kcpps",
        "language_url": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/blob/main/Qwen2-VL-2B-Instruct-Q6_K.gguf",
        "mmproj_url": "https://huggingface.co/bartowski/Qwen2-VL-2B-Instruct-GGUF/blob/main/mmproj-Qwen2-VL-2B-Instruct-f16.gguf",
        "description": "Smallest and fastest release of Qwen2-VL. If you are unsure CHOOSE THIS if you can't fit Gemma-3 4B.",
        "size_mb": 3120,
        "adapter": "chatml"
    },
    {
        "model": "Gemma-3 4B (6bit)",
        "config": "gemma-3-4b-q6.kcpps",
        "language_url": "https://huggingface.co/bartowski/google_gemma-3-4b-it-GGUF/blob/main/google_gemma-3-4b-it-Q6_K.gguf",
        "mmproj_url": "https://huggingface.co/bartowski/google_gemma-3-4b-it-GGUF/blob/main/mmproj-google_gemma-3-4b-it-f16.gguf",
        "description": "Gemma-3 4B is the smallest and fastest release of Gemma-3. If you are unsure, CHOOSE THIS if it fits in your VRAM.",
        "size_mb": 4800,
        "adapter": "gemma-3"
    },
    
    {
        "model": "Qwen2.5-VL 7B (6bit)",
        "config": "qwen2.5-vl-7b-q6.kcpps",
        "language_url": "https://huggingface.co/bartowski/Qwen_Qwen2.5-VL-7B-Instruct-GGUF/blob/main/Qwen_Qwen2.5-VL-7B-Instruct-Q6_K.gguf",
        "mmproj_url": "https://huggingface.co/bartowski/Qwen_Qwen2.5-VL-7B-Instruct-GGUF/blob/main/mmproj-Qwen_Qwen2.5-VL-7B-Instruct-f16.gguf",
        "description": "Mid size release of Qwen2.5-VL.",
        "size_mb": 9120,
        "adapter": "chatml"
    },
    {
        "model": "Qwen2.5-VL 7B (4bit)",
        "config": "qwen2-vl-7b-q4.kcpps",
        "language_url": "https://huggingface.co/bartowski/Qwen_Qwen2.5-VL-7B-Instruct-GGUF/blob/main/Qwen_Qwen2.5-VL-7B-Instruct-Q4_K_M.gguf",
        "mmproj_url": "https://huggingface.co/bartowski/Qwen_Qwen2.5-VL-7B-Instruct-GGUF/blob/main/mmproj-Qwen_Qwen2.5-VL-7B-Instruct-f16.gguf",
        "description": "Mid size release of Qwen2.5-VL meant to fit in 8GB of VRAM",
        "size_mb": 7200,
        "adapter": "chatml"
    },
    {
        "model": "MiniCPM-V 2.6 (4 bit)",
        "config": "minicpm-v-2_6-q4.kcpps",
        "language_url": "https://huggingface.co/bartowski/MiniCPM-V-2_6-GGUF/blob/main/MiniCPM-V-2_6-Q4_K_M.gguf",
        "mmproj_url": "https://huggingface.co/bartowski/MiniCPM-V-2_6-GGUF/blob/main/mmproj-MiniCPM-V-2_6-f16.gguf",
        "description": "Very good image model based on Qwen2",
        "size_mb": 6800,
        "adapter": "chatml"
    },
	{
        "model": "Pixtral Captioner 12B (4bit)",
        "config": "pixtral-captioner-12b-q4.kcpps",
        "language_url": "https://huggingface.co/Hyphonical/Pixtral-12B-Captioner-Relaxed-Q4_K_M-GGUF/blob/main/pixtral-12b-captioner-relaxed-q4_k_m.gguf",
        "mmproj_url": "https://huggingface.co/mradermacher/pixtral-12b-GGUF/blob/main/pixtral-12b.mmproj-f16.gguf",
        "description": "Mistralai's Pixtral fine-tuned for captioning (uncensored)",
        "size_mb": 9800,
        "adapter": "mistral"
    }, 
    {
        "model": "Gemma-3 12B (4bit)",
        "config": "gemma-3-12b-q4.kcpps",
        "language_url": "https://huggingface.co/bartowski/google_gemma-3-12b-it-GGUF/blob/main/google_gemma-3-12b-it-Q4_K_S.gguf",
        "mmproj_url": "https://huggingface.co/bartowski/google_gemma-3-12b-it-GGUF/blob/main/mmproj-google_gemma-3-12b-it-f16.gguf",
        "description": "Medium size release of Gemma-3",
        "size_mb": 9780,
        "adapter": "gemma-3"
    }    
]
